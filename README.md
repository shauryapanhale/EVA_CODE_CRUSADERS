# EVA â€“ Enhanced Virtual Assistant (Windows) ğŸ¤–

A JARVISâ€‘style desktop assistant for Windows that listens for â€œJARVISâ€, understands natural commands locally using Ollama (Mistral LLM), analyzes the screen with Omniparser + Gemini, and executes actions via keyboard/mouse/system executors.

---

## âœ… What you need (install these)

1) Windows 10/11 (64â€‘bit), microphone enabled
2) Python 3.8+ (check â€œAdd Python to PATHâ€ during install)
3) Git for Windows
4) C compiler (MinGW GCC) for building executors
5) Ollama (local LLM runtime)
6) Mistral model for Ollama (downloaded inside Ollama)
7) Google Gemini API key (free) for screen analysis
8) Stable internet only for the oneâ€‘time downloads above

---

## ğŸ“¦ Quick setup (oneâ€‘time)

Open PowerShell and follow these steps.

1) Clone and enter the project
git clone https://github.com/shauryapanhale/EVA_CODE_CRUSADERS.git
cd EVA_CODE_CRUSADERS

2) Create and activate a virtual environment
python -m venv eva_env
eva_env\Scripts\activate

3) Install Python dependencies
pip install -r requirements.txt

4) Install Ollama (download and run the Windows installer)
- Go to https://ollama.ai and install
- Then pull the Mistral model:
ollama pull mistral
Verify:
ollama list
You should see: mistral:latest

5) Add your Gemini API key
- Get a free key at: https://makersuite.google.com/app/apikey
- Create a file named .env in the project root with:
GEMINI_API_KEY=your_key_here
Alternatively you can put the key into config.py if thatâ€™s how your code reads it.

6) Install MinGW (GCC) and add to PATH (only if you donâ€™t have gcc)
- Download MinGW from SourceForge (mingw-get installer)
- During installation, select mingw32-gcc
- Add C:\MinGW\bin to your PATH
- Open a fresh PowerShell and confirm:
gcc --version

7) Build the C executors (if not already present)
cd execution\c_executors
gcc -shared -o executor.dll executor.c system_commands.c -luser32 -lgdi32
cd ....

8) (Optional) Preâ€‘download Whisper model to avoid firstâ€‘run delay
python -c "from faster_whisper import WhisperModel; WhisperModel('large-v2', device='cpu', compute_type='int8')"

---

## â–¶ï¸ How to run (every time)

You need two terminals.

Terminal A â€“ start Ollama service:
ollama serve
Keep it running.

Terminal B â€“ run EVA:

cd EVA_CODE_CRUSADERS
eva_env\Scripts\activate
python main.py

You should see it listening for the wake word.

---

## ğŸ—£ï¸ What to say

- Activate: â€œJARVISâ€
- System: â€œset volume to 50â€, â€œincrease brightnessâ€, â€œmuteâ€
- Apps: â€œopen spotifyâ€, â€œopen chromeâ€, â€œopen calculatorâ€, â€œclose windowâ€
- Media: â€œplay musicâ€, â€œpauseâ€, â€œnext songâ€, â€œprevious trackâ€
- Web: â€œsearch for python tutorialsâ€, â€œgo to youtubeâ€
- End: â€œgoodbye jarvisâ€

Tips:
- Speak clearly after the wake word
- Leave ~1 sec pause after â€œJARVISâ€ before your command
- Keep commands within 10 seconds (default recording window)

---

## ğŸ§± Project structure (important folders)

EVA_CODE_CRUSADERS/
â”œâ”€ main.py # Entry point
â”œâ”€ config.py # Configs (can point to .env)
â”œâ”€ requirements.txt # Python deps
â”œâ”€ .env # Put GEMINI_API_KEY here (do not commit)
â”œâ”€ execution/
â”‚ â”œâ”€ action_router.py # Routes actions to executors
â”‚ â”œâ”€ system_executor.py # Volume/brightness/power
â”‚ â””â”€ c_executors/ # Low-level executors (C)
â”‚ â”œâ”€ executor.c
â”‚ â”œâ”€ system_commands.c
â”‚ â””â”€ executor.dll # Built artifact
â”œâ”€ models/
â”‚ â”œâ”€ semantic_classifier.py # Uses Ollama (Mistral) for intent
â”‚ â”œâ”€ command_processor.py # Validates, extracts entities
â”‚ â””â”€ (fallback models) # If you keep classical classifiers
â”œâ”€ speech/
â”‚ â”œâ”€ speech_to_text.py # Faster Whisper
â”‚ â”œâ”€ text_to_speech.py
â”‚ â””â”€ wake_word_detector.py # Porcupine
â”œâ”€ vision/
â”‚ â”œâ”€ screen_analyzer.py # Gemini + Omniparser glue
â”‚ â”œâ”€ screenshot_handler.py
â”‚ â””â”€ omniparser_executor.py
â”œâ”€ utils/ # Logger, helpers, etc.
â””â”€ weights/ # Model weights (e.g., Omniparser)

---

## ğŸ”§ Troubleshooting

- Ollama error 404
  - Cause: Model not pulled or service not running
  - Fix:
    ```
    ollama pull mistral
    ollama serve
    ```

- â€œgcc not foundâ€
  - Install MinGW, add C:\MinGW\bin to PATH, reopen PowerShell, run `gcc --version`

- Wake word not detected
  - Check microphone input device in Windows Sound Settings
  - Reduce background noise, speak clearly â€œJARVISâ€

- Very slow first run
  - First time downloads and model warmâ€‘up; subsequent runs are faster
  - Preâ€‘download Whisper as shown above

- Volume doesnâ€™t change
  - Ensure system_executorâ€™s volume path uses pycaw or fallback is enabled
  - Optionally: `pip install pycaw`

- â€œModule not foundâ€
  - Make sure the venv is active and `pip install -r requirements.txt` completed

---

## ğŸ›¡ï¸ Notes on privacy

- Speech recognition, LLM, and execution run locally
- Internet is only required for oneâ€‘time downloads (models, dependencies) and Gemini vision calls
- Keep your API keys in `.env` and do not commit them

---

## ğŸ§ª Quick smoke test

1) Start Ollama:
ollama serve
2) Run EVA:
eva_env\Scripts\activate
python main.py
3) Say: â€œJARVIS â€¦ open notepadâ€
4) Say: â€œset volume to 30â€
5) Say: â€œgoodbye jarvisâ€

You should see the app open, volume adjust, and session end cleanly.

---

## ğŸ“„ License

MIT â€“ free to use and modify for learning and projects.
If you also want the exact git push steps for your repo, run these in PowerShell from your project folder:
git remote set-url origin https://github.com/shauryapanhale/EVA_CODE_CRUSADERS.git
git add .
git commit -m "ğŸ“ Add complete README and setup instructions"
git push -u origin main
If the branch is not main yet:
git branch -M main
git push -u origin main --force


